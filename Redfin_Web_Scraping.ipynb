{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5469db27",
   "metadata": {},
   "source": [
    "## Redfin Web Scraping Project\n",
    "\n",
    "#### Minh (Mark) Le"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ef869b",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "781fa96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import time\n",
    "import datetime\n",
    "from time import sleep\n",
    "import smtplib\n",
    "import random\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8f56e696",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import Selenium\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.remote.webelement import WebElement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7095dcdd",
   "metadata": {},
   "source": [
    "### Parsing with BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf70754a",
   "metadata": {},
   "source": [
    "#### Get Information about Properties in Greater Vancouver Area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f1fe28",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Looping to pages to get information about properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6f78f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "## get number of pages each city name\n",
    "## split by comma and strip the item in the listing\n",
    "def get_number_of_pages(city_name):\n",
    "    formatted_city_name = city_name.split(',')\n",
    "    formatted_province = formatted_city_name[1].strip().lower()\n",
    "    formatted_city = formatted_city_name[0].strip().lower()\n",
    "    url = f'https://www.redfin.ca/{formatted_province}/{formatted_city}'\n",
    "    header = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.114 Safari/537.36\",\n",
    "        \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9\",\n",
    "        \"Accept-Language\": \"en-US,en;q=0.9\",\n",
    "        'referer': 'https://www.redfin.ca/',\n",
    "}\n",
    "    html = requests.get(url=url,headers=header)\n",
    "    soup = BeautifulSoup(html.content, 'lxml')\n",
    "    no_pages = int(soup.find('div', {'class': 'viewingPage'}).get_text().split()[4])\n",
    "\n",
    "    return no_pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def get_page_url(city_name):\n",
    "    formatted_city_name = city_name.split(',')\n",
    "    formatted_province = formatted_city_name[1].strip().lower()\n",
    "    formatted_city = formatted_city_name[0].strip().lower()\n",
    "\n",
    "    for page_number in range(0, get_number_of_pages(city_name)+1):\n",
    "        url = f'https://www.redfin.ca/{formatted_province}/{formatted_city}/'\n",
    "        header = {\n",
    "            \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.114 Safari/537.36\",\n",
    "            \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9\",\n",
    "            \"Accept-Language\": \"en-US,en;q=0.9\",\n",
    "            'referer': 'https://www.redfin.ca/',\n",
    "}\n",
    "\n",
    "\n",
    "    return url"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Navigating to listings"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# copy the property listing and use selenium to navigate each page instead of using looping"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def property_listing(city_name):\n",
    "    url = get_page_url(city_name)\n",
    "    driver = webdriver.Chrome('chromedriver.exe')\n",
    "    driver.get(url)\n",
    "\n",
    "    for page_number in range(1, get_number_of_pages(city_name) + 1):\n",
    "        print(page_number)\n",
    "        print(get_page_url(city_name))\n",
    "\n",
    "        header = {\n",
    "        # use same headers as a popular web browser (Chrome on Windows in this case)\n",
    "            \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.114 Safari/537.36\",\n",
    "            \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9\",\n",
    "            \"Accept-Language\": \"en-US,en;q=0.9\",\n",
    "        }\n",
    "        result = requests.get(url = url, headers = header)\n",
    "        soup = BeautifulSoup(result.content, 'lxml')\n",
    "        # page_source = driver.page_source\n",
    "        # soup = BeautifulSoup(page_source)\n",
    "        sleep(random.randint(20,30))\n",
    "\n",
    "        listings = soup.find_all('div', {'class': 'v2 interactive'})\n",
    "        data = {\n",
    "            'price_list': [],\n",
    "            'address': [],\n",
    "            'home_stat': []\n",
    "        }\n",
    "        for listing in listings:\n",
    "            try:\n",
    "                price_list = listing.find('span', {'class': 'homecardV2Price'}).get_text()\n",
    "                address = listing.find('div', {'class': 'link-and-anchor'}).text\n",
    "                home_stat = listing.find('div', {'class': 'HomeStatsV2 font-size-small'}).text\n",
    "                data['price_list'].append(price_list)\n",
    "                data['address'].append(address)\n",
    "                data['home_stat'].append(home_stat)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        hdr = False  if os.path.isfile(f'data_{city_name}.csv') else True\n",
    "        df = pd.DataFrame(data).to_csv(f'data_{city_name}.csv', mode='a',header=hdr, index=False, encoding='utf-8')\n",
    "        sleep(random.randint(20,30))\n",
    "\n",
    "        # driver.find_element(By.XPATH,'//*[@id=\"results-display\"]/div[5]/div/div[3]/button[2]')\n",
    "        # location = e.location\n",
    "        # y = location.get('y')\n",
    "\n",
    "        # use Selenium to scroll down and click next page\n",
    "        driver.execute_script(\"window.scrollTo(0, window.scrollY + 15400);\")\n",
    "        sleep(random.randint(10, 15))\n",
    "        # e.click()\n",
    "        if page_number == get_number_of_pages(city_name):\n",
    "            break\n",
    "        else:\n",
    "            driver.find_element(By.XPATH,'//*[@id=\"results-display\"]/div[5]/div/div[3]/button[2]').click()\n",
    "            sleep(random.randint(20, 30))\n",
    "            url = driver.current_url"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "id": "b3bae4f3",
   "metadata": {},
   "source": [
    "#### User defines city name and province of the area they want to scrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b252d2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "city_name_list = ['Vancouver, BC', 'Toronto, ON']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "# no_pages = []\n",
    "#\n",
    "# [no_pages.append(get_number_of_pages(city_name)) for index, city_name in enumerate(city_name_list)]\n",
    "#\n",
    "# # for index, job_name in enumerate(jobs_titles_list):\n",
    "# #     no_pages.append(get_no_pages(job_name))\n",
    "#\n",
    "# no_pages[index]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Off market - Sold property"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_number_of_pages_sold(city_name):\n",
    "    formatted_city_name = city_name.split(',')\n",
    "    formatted_province = formatted_city_name[1].strip().lower()\n",
    "    formatted_city = formatted_city_name[0].strip().lower()\n",
    "    url = f'https://www.redfin.ca/{formatted_province}/{formatted_city}/filter/include=sold-1yr/'\n",
    "    header = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.114 Safari/537.36\",\n",
    "        \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9\",\n",
    "        \"Accept-Language\": \"en-US,en;q=0.9\",\n",
    "        'referer': 'https://www.redfin.ca/',\n",
    "}\n",
    "    html = requests.get(url=url,headers=header)\n",
    "    soup = BeautifulSoup(html.content, 'lxml')\n",
    "    no_pages = int(soup.find('div', {'class': 'viewingPage'}).get_text().split()[4])\n",
    "\n",
    "    return no_pages"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def sold_property(city_name):\n",
    "    driver = webdriver.Chrome('chromedriver.exe')\n",
    "    header = {\n",
    "            # use same headers as a popular web browser (Chrome on Windows in this case)\n",
    "                \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.114 Safari/537.36\",\n",
    "                \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9\",\n",
    "                \"Accept-Language\": \"en-US,en;q=0.9\",\n",
    "            }\n",
    "    mainpage = 'https://www.redfin.ca/'\n",
    "    driver.get(mainpage)\n",
    "    sleep(random.randint(10, 15))\n",
    "    search_bar = driver.find_element(By.XPATH, '//*[@id=\"search-box-input\"]')\n",
    "    search_bar.clear()\n",
    "    search_bar.send_keys(city_name)\n",
    "    search_bar.send_keys(Keys.RETURN)\n",
    "    sleep(random.randint(5, 8))\n",
    "    #click drop down menu\n",
    "    delement = driver.find_element(By.XPATH, '//*[@id=\"sidepane-header\"]/div/div[1]/form/div[1]/div').click()\n",
    "    sleep(random.randint(2, 5))\n",
    "    #click sold\n",
    "    delement = driver.find_element(By.XPATH, '//*[@id=\"solds-expandable-segment\"]/div[1]/div/span/span/div/div/label/span').click()\n",
    "    sleep(random.randint(2, 5))\n",
    "    #choosing 1 year\n",
    "    delement = driver.find_element(By.XPATH, '//*[@id=\"solds-expandable-segment\"]/div[1]/div/div[5]/span/span/div/div/label/span').click()\n",
    "    #click done\n",
    "    delement = driver.find_element(By.XPATH, '//*[@id=\"sidepane-header\"]/div/div[1]/form/div[1]/div[2]/div[1]/div/div[2]/div/button[2]').click()\n",
    "    sleep(random.randint(3, 5))\n",
    "    #click login button\n",
    "    delement = driver.find_element(By.XPATH, '//*[@id=\"header-content\"]/header[2]/div[2]/div[5]/button').click()\n",
    "    sleep(random.randint(3, 5))\n",
    "    login = driver.find_element(By.XPATH, '/html/body/div[8]/div/div[2]/div/div/div/div[2]/div/div/div/div[1]/div/div/form/div/div[1]/div/span/span').click()\n",
    "    sleep(random.randint(1, 2))\n",
    "    driver.find_element(By.XPATH,'/html/body/div[8]/div/div[2]/div/div/div/div[2]/div/div/div/div[1]/div/div/form/div/div[1]/div/span/span/div/input').send_keys(username)\n",
    "    sleep(random.randint(1, 2))\n",
    "    #click continue with email button\n",
    "    driver.find_element(By.XPATH,'/html/body/div[8]/div/div[2]/div/div/div/div[2]/div/div/div/div[1]/div/div/form/div/div[1]/button').click()\n",
    "    sleep(random.randint(3, 5))\n",
    "    #click sign in with email instead\n",
    "    driver.find_element(By.XPATH, '/html/body/div[8]/div/div[2]/div/div/div/div[2]/div/div/div/div[1]/div/div/div[3]/button').click()\n",
    "    sleep(random.randint(1, 2))\n",
    "    driver.find_element(By.XPATH, '/html/body/div[8]/div/div[2]/div/div/div/div[2]/div/div/div/div[1]/div/div/form/div/div[2]/span/span/div/input').send_keys(password)\n",
    "    sleep(random.randint(1, 2))\n",
    "    #click continue with email\n",
    "    driver.find_element(By.XPATH, '/html/body/div[8]/div/div[2]/div/div/div/div[2]/div/div/div/div[1]/div/div/form/div/div[4]/button').click()\n",
    "\n",
    "    for page_number in range(1, get_number_of_pages_sold(city_name) + 1):\n",
    "        #print(page_number)\n",
    "        #print(get_page_url(city_name))\n",
    "\n",
    "        header = {\n",
    "        # use same headers as a popular web browser (Chrome on Windows in this case)\n",
    "            \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.114 Safari/537.36\",\n",
    "            \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9\",\n",
    "            \"Accept-Language\": \"en-US,en;q=0.9\",\n",
    "        }\n",
    "        result = requests.get(url = driver.current_url, headers = header)\n",
    "        soup = BeautifulSoup(result.content, 'lxml')\n",
    "        # page_source = driver.page_source\n",
    "        # soup = BeautifulSoup(page_source)\n",
    "        sleep(random.randint(20,30))\n",
    "\n",
    "        listings = soup.find_all('div', {'class': 'v2 interactive'})\n",
    "        data = {\n",
    "            'price_list': [],\n",
    "            'address': [],\n",
    "            'home_stat': [],\n",
    "            'date_sold': []\n",
    "        }\n",
    "        for listing in listings:\n",
    "            try:\n",
    "                price_list = listing.find('span', {'class': 'homecardV2Price'}).get_text()\n",
    "                address = listing.find('div', {'class': 'link-and-anchor'}).text\n",
    "                home_stat = listing.find('div', {'class': 'HomeStatsV2 font-size-small'}).text\n",
    "                date_sold = listing.find('div', {'class': 'Pill Pill--sold padding-vert-smallest padding-horiz-smaller font-size-smaller font-weight-bold font-color-white HomeSash margin-right-smaller margin-top-smallest'}).text\n",
    "                data['price_list'].append(price_list)\n",
    "                data['address'].append(address)\n",
    "                data['home_stat'].append(home_stat)\n",
    "                data['date_sold'].append(date_sold)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        hdr = False  if os.path.isfile(f'data_sold_property_{city_name}.csv') else True\n",
    "        df_sold = pd.DataFrame(data).to_csv(f'data_{city_name}.csv', mode='a',header=hdr, index=False, encoding='utf-8')\n",
    "        #df_sold = pd.DataFrame(data).to_csv(f'data_sold_property_{city_name}.csv', mode='a',header=hdr, index=False, encoding='utf-8')\n",
    "        #df_sold = df_sold.transpose()\n",
    "        # df_sold= pd.DataFrame.from_dict(df_sold, orient='index')\n",
    "        # df_sold = df_sold.transpose()\n",
    "        sleep(random.randint(20,30))\n",
    "\n",
    "        # driver.find_element(By.XPATH,'//*[@id=\"results-display\"]/div[5]/div/div[3]/button[2]')\n",
    "        # location = e.location\n",
    "        # y = location.get('y')\n",
    "\n",
    "        # use Selenium to scroll down and click next page\n",
    "        driver.execute_script(\"window.scrollTo(0, window.scrollY + 15400);\")\n",
    "        sleep(random.randint(10, 15))\n",
    "        # e.click()\n",
    "        if page_number == get_number_of_pages_sold(city_name):\n",
    "            break\n",
    "        else:\n",
    "            driver.find_element(By.XPATH,'//*[@id=\"results-display\"]/div[5]/div/div[3]/button[2]').click()\n",
    "            sleep(random.randint(20, 30))\n",
    "            url = driver.current_url"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [],
   "source": [
    "#Input your credential of Redfin Login here\n",
    "username = 'mark.mqle@gmail.com'\n",
    "password = 'Lequangminh1509!'"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\markm\\AppData\\Local\\Temp\\ipykernel_42304\\3817289563.py:2: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver = webdriver.Chrome('chromedriver.exe')\n"
     ]
    },
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: 'data_Toronto, ON.csv'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mPermissionError\u001B[0m                           Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_42304\\3972197276.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      2\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0mcity_name\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mcity_name_list\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      3\u001B[0m     \u001B[1;31m#property_listing(city_name)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 4\u001B[1;33m     \u001B[0msold_property\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mcity_name\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      5\u001B[0m     \u001B[1;32mcontinue\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_42304\\3817289563.py\u001B[0m in \u001B[0;36msold_property\u001B[1;34m(city_name)\u001B[0m\n\u001B[0;32m     81\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     82\u001B[0m         \u001B[0mhdr\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;32mFalse\u001B[0m  \u001B[1;32mif\u001B[0m \u001B[0mos\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpath\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0misfile\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34mf'data_sold_property_{city_name}.csv'\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32melse\u001B[0m \u001B[1;32mTrue\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 83\u001B[1;33m         \u001B[0mdf_sold\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mpd\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mDataFrame\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdata\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mto_csv\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34mf'data_{city_name}.csv'\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmode\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;34m'a'\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mheader\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mhdr\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mindex\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mFalse\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     84\u001B[0m         \u001B[1;31m#df_sold = pd.DataFrame(data).to_csv(f'data_sold_property_{city_name}.csv', mode='a',header=hdr, index=False, encoding='utf-8')\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     85\u001B[0m         \u001B[1;31m#df_sold = df_sold.transpose()\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001B[0m in \u001B[0;36mto_csv\u001B[1;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, line_terminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001B[0m\n\u001B[0;32m   3549\u001B[0m         )\n\u001B[0;32m   3550\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 3551\u001B[1;33m         return DataFrameRenderer(formatter).to_csv(\n\u001B[0m\u001B[0;32m   3552\u001B[0m             \u001B[0mpath_or_buf\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   3553\u001B[0m             \u001B[0mline_terminator\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mline_terminator\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\formats\\format.py\u001B[0m in \u001B[0;36mto_csv\u001B[1;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, line_terminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001B[0m\n\u001B[0;32m   1178\u001B[0m             \u001B[0mformatter\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfmt\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1179\u001B[0m         )\n\u001B[1;32m-> 1180\u001B[1;33m         \u001B[0mcsv_formatter\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msave\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1181\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1182\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mcreated_buffer\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\formats\\csvs.py\u001B[0m in \u001B[0;36msave\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    239\u001B[0m         \"\"\"\n\u001B[0;32m    240\u001B[0m         \u001B[1;31m# apply compression and byte/text conversion\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 241\u001B[1;33m         with get_handle(\n\u001B[0m\u001B[0;32m    242\u001B[0m             \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfilepath_or_buffer\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    243\u001B[0m             \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmode\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\common.py\u001B[0m in \u001B[0;36mget_handle\u001B[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001B[0m\n\u001B[0;32m    784\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mioargs\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mencoding\u001B[0m \u001B[1;32mand\u001B[0m \u001B[1;34m\"b\"\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mioargs\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmode\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    785\u001B[0m             \u001B[1;31m# Encoding\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 786\u001B[1;33m             handle = open(\n\u001B[0m\u001B[0;32m    787\u001B[0m                 \u001B[0mhandle\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    788\u001B[0m                 \u001B[0mioargs\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmode\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mPermissionError\u001B[0m: [Errno 13] Permission denied: 'data_Toronto, ON.csv'"
     ]
    }
   ],
   "source": [
    "## Main loop for scraping\n",
    "for city_name in city_name_list:\n",
    "    #property_listing(city_name)\n",
    "    sold_property(city_name)\n",
    "    continue"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Exploratory Data Analysis"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dfdict = {}\n",
    "for city_name in city_name_list:\n",
    "    exec(f'df_{city_name} = create_df(\"data_{city_name}.csv\")') # create a dataframe for each job\n",
    "    exec(f'dfdict[city_name] = df_{city_name}') # create a dict that contains the dataframes"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
